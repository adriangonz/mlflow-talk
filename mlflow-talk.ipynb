{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLFlow and Seldon\n",
    "\n",
    "End to end example integrating MLFlow and Seldon, with A/B testing of the models.\n",
    "We also cover how to explain the model's predictions with Alibi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Training\n",
    "\n",
    "This first section covers how to train models using MLFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLproject\n",
    "\n",
    "The MLproject file defines:\n",
    "- The environment where the training runs.\n",
    "- The hyperparameters that can be tweaked. In our case, these are $\\{\\alpha, l_{1}\\}$.\n",
    "- The interface to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mname\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34mmlflow\u001b[39;49;00m\u001b[31m-\u001b[39;49;00m\u001b[34mtalk\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mconda_env\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34mconda\u001b[39;49;00m\u001b[31m.\u001b[39;49;00m\u001b[34myaml\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mentry_points\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m\n",
      "  \u001b[34mmain\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m\n",
      "    \u001b[34mparameters\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m\n",
      "      \u001b[34malpha\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34mfloat\u001b[39;49;00m\n",
      "      \u001b[34ml1_ratio\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[31m{\u001b[39;49;00m\u001b[34mtype\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34mfloat\u001b[39;49;00m\u001b[31m,\u001b[39;49;00m \u001b[34mdefault\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34m0.1\u001b[39;49;00m\u001b[31m}\u001b[39;49;00m\n",
      "    \u001b[34mcommand\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[33m\"python train.py {alpha} {l1_ratio}\"\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "!ccat ./training/MLproject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows us to have a single command to train the model. \n",
    "\n",
    "``` bash\n",
    "$ mlflow run ./training -P alpha=... -P l1_ratio=...\n",
    "```\n",
    "\n",
    "For our example, we will train two versions of the model, which we'll later compare using A/B testing.\n",
    "\n",
    "- $M_{1}$ with $\\alpha = 0.5$\n",
    "- $M_{2}$ with $\\alpha = 0.75$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019/10/02 21:45:18 INFO mlflow.projects: === Created directory /tmp/tmpnwewijx3 for downloading remote URIs passed to arguments of type 'path' ===\n",
      "2019/10/02 21:45:18 INFO mlflow.projects: === Running command 'source /opt/miniconda3/bin/../etc/profile.d/conda.sh && conda activate mlflow-1ecba04797edb7e7f7212d429debd9b664c31651 1>&2 && python train.py 0.5 0.1' in run with ID '9735daaa839b4934abeaf2768fc1be93' === \n",
      "Elasticnet model (alpha=0.500000, l1_ratio=0.100000):\n",
      "  RMSE: 0.7947931019036529\n",
      "  MAE: 0.6189130834228138\n",
      "  R2: 0.18411668718221819\n",
      "2019/10/02 21:45:19 INFO mlflow.projects: === Run (ID '9735daaa839b4934abeaf2768fc1be93') succeeded ===\n"
     ]
    }
   ],
   "source": [
    "!mlflow run ./training -P alpha=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019/10/02 15:13:04 INFO mlflow.projects: === Created directory /tmp/tmpm5k6nyxe for downloading remote URIs passed to arguments of type 'path' ===\n",
      "2019/10/02 15:13:04 INFO mlflow.projects: === Running command 'source /opt/miniconda3/bin/../etc/profile.d/conda.sh && conda activate mlflow-1ecba04797edb7e7f7212d429debd9b664c31651 1>&2 && python train.py 0.75 0.1' in run with ID '272fe7c7ffb345f089cfd6285772ef69' === \n",
      "Elasticnet model (alpha=0.750000, l1_ratio=0.100000):\n",
      "  RMSE: 0.8037846644965104\n",
      "  MAE: 0.6221040103321569\n",
      "  R2: 0.16555194969389364\n",
      "2019/10/02 15:13:05 INFO mlflow.projects: === Run (ID '272fe7c7ffb345f089cfd6285772ef69') succeeded ===\n"
     ]
    }
   ],
   "source": [
    "!mlflow run ./training -P alpha=1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLtrack\n",
    "\n",
    "The `train.py` script uses the `mlflow.log_param()` and `mlflow.log_metric()` commands to track each experiment. These are part of the `MLtrack` API, which tracks experiments parameters and results. These can be stored on a remote server, which can then be shared across the entire team. However, on our example we will store these locally on a `mlruns` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mmlruns\u001b[00m\n",
      "└── \u001b[01;34m0\u001b[00m\n",
      "    ├── \u001b[01;34m272fe7c7ffb345f089cfd6285772ef69\u001b[00m\n",
      "    │   ├── \u001b[01;34martifacts\u001b[00m\n",
      "    │   │   └── \u001b[01;34mmodel\u001b[00m\n",
      "    │   │       ├── conda.yaml\n",
      "    │   │       ├── MLmodel\n",
      "    │   │       └── model.pkl\n",
      "    │   ├── meta.yaml\n",
      "    │   ├── \u001b[01;34mmetrics\u001b[00m\n",
      "    │   │   ├── mae\n",
      "    │   │   ├── r2\n",
      "    │   │   └── rmse\n",
      "    │   ├── \u001b[01;34mparams\u001b[00m\n",
      "    │   │   ├── alpha\n",
      "    │   │   └── l1_ratio\n",
      "    │   └── \u001b[01;34mtags\u001b[00m\n",
      "    │       ├── mlflow.project.backend\n",
      "    │       ├── mlflow.project.entryPoint\n",
      "    │       ├── mlflow.project.env\n",
      "    │       ├── mlflow.source.name\n",
      "    │       ├── mlflow.source.type\n",
      "    │       └── mlflow.user\n",
      "    ├── \u001b[01;34m29876d07ee1b48d7b460cf38366eda06\u001b[00m\n",
      "    │   ├── \u001b[01;34martifacts\u001b[00m\n",
      "    │   │   └── \u001b[01;34mmodel\u001b[00m\n",
      "    │   │       ├── conda.yaml\n",
      "    │   │       ├── MLmodel\n",
      "    │   │       └── model.pkl\n",
      "    │   ├── meta.yaml\n",
      "    │   ├── \u001b[01;34mmetrics\u001b[00m\n",
      "    │   │   ├── mae\n",
      "    │   │   ├── r2\n",
      "    │   │   └── rmse\n",
      "    │   ├── \u001b[01;34mparams\u001b[00m\n",
      "    │   │   ├── alpha\n",
      "    │   │   └── l1_ratio\n",
      "    │   └── \u001b[01;34mtags\u001b[00m\n",
      "    │       ├── mlflow.project.backend\n",
      "    │       ├── mlflow.project.entryPoint\n",
      "    │       ├── mlflow.project.env\n",
      "    │       ├── mlflow.source.name\n",
      "    │       ├── mlflow.source.type\n",
      "    │       └── mlflow.user\n",
      "    └── meta.yaml\n",
      "\n",
      "13 directories, 31 files\n"
     ]
    }
   ],
   "source": [
    "!tree mlruns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run `mlflow ui` to show these visually.\n",
    "\n",
    "```bash\n",
    "$ mlflow ui\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLFlow UI](./images/mlflow-ui.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLmodel\n",
    "\n",
    "The `MLmodel` file allows us to version and share models easily. Below we can see an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34martifact_path\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34mmodel\u001b[39;49;00m\n",
      "\u001b[34mflavors\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m\n",
      "  \u001b[34mpython_function\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m\n",
      "    \u001b[34mdata\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34mmodel\u001b[39;49;00m\u001b[31m.\u001b[39;49;00m\u001b[34mpkl\u001b[39;49;00m\n",
      "    \u001b[34menv\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34mconda\u001b[39;49;00m\u001b[31m.\u001b[39;49;00m\u001b[34myaml\u001b[39;49;00m\n",
      "    \u001b[34mloader_module\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34mmlflow\u001b[39;49;00m\u001b[31m.\u001b[39;49;00m\u001b[34msklearn\u001b[39;49;00m\n",
      "    \u001b[34mpython_version\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34m3.6\u001b[39;49;00m\u001b[34m.9\u001b[39;49;00m\n",
      "  \u001b[34msklearn\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m\n",
      "    \u001b[34mpickled_model\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34mmodel\u001b[39;49;00m\u001b[31m.\u001b[39;49;00m\u001b[34mpkl\u001b[39;49;00m\n",
      "    \u001b[34mserialization_format\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34mcloudpickle\u001b[39;49;00m\n",
      "    \u001b[34msklearn_version\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34m0.19\u001b[39;49;00m\u001b[34m.1\u001b[39;49;00m\n",
      "\u001b[34mrun_id\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34m5\u001b[39;49;00m\u001b[34ma6be5a1ef844783a50a6577745dbdc3\u001b[39;49;00m\n",
      "\u001b[34mutc_time_created\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[33m'2019-10-02 14:21:15.783806'\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "!ccat ./mlruns/0/5a6be5a1ef844783a50a6577745dbdc3/artifacts/model/MLmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above the `MLmodel` keeps track, between others, of\n",
    "\n",
    "- The experiment id, `5a6be5a1ef844783a50a6577745dbdc3`\n",
    "- Date \n",
    "- Version of `sklearn` \n",
    "- How the model was stored\n",
    "\n",
    "As we shall see shortly, the pre-packaged Seldon's model server will use this file to serve this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Serving\n",
    "\n",
    "To serve this model we will use Seldon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Set up\n",
    "\n",
    "Before anything, we will first set up the `k8s` cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create k8s cluster\n",
    "\n",
    "Firstly, we will create a cluster using [kind](https://kind.sigs.k8s.io)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating cluster \"kind\" ...\n",
      " ✓ Ensuring node image (kindest/node:v1.15.3) 🖼\n",
      " ✓ Preparing nodes 📦 \n",
      " ✓ Creating kubeadm config 📜 \n",
      " ✓ Starting control-plane 🕹️ \n",
      " ✓ Installing CNI 🔌 \n",
      " ✓ Installing StorageClass 💾 \n",
      "Cluster creation complete. You can now use the cluster with:\n",
      "\n",
      "export KUBECONFIG=\"$(kind get kubeconfig-path --name=\"kind\")\"\n",
      "kubectl cluster-info\n"
     ]
    }
   ],
   "source": [
    "!kind create cluster\n",
    "!export KUBECONFIG=\"$(kind get kubeconfig-path --name=kind)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then install Helm and a corresponding service account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$HELM_HOME has been configured at /home/agm/.helm.\n",
      "\n",
      "Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster.\n",
      "\n",
      "Please note: by default, Tiller is deployed with an insecure 'allow unauthenticated users' policy.\n",
      "To prevent this, run `helm init` with the --tiller-tls-verify flag.\n",
      "For more information on securing your installation see: https://docs.helm.sh/using_helm/#securing-your-helm-installation\n",
      "Waiting for deployment spec update to be observed...\n",
      "Waiting for deployment spec update to be observed...\n",
      "Waiting for deployment \"tiller-deploy\" rollout to finish: 0 out of 1 new replicas have been updated...\n",
      "Waiting for deployment \"tiller-deploy\" rollout to finish: 0 of 1 updated replicas are available...\n",
      "deployment \"tiller-deploy\" successfully rolled out\n",
      "serviceaccount/tiller created\n",
      "clusterrolebinding.rbac.authorization.k8s.io/tiller-cluster-rule created\n",
      "deployment.extensions/tiller-deploy patched\n"
     ]
    }
   ],
   "source": [
    "!helm init --history-max 200\n",
    "!kubectl rollout status deploy/tiller-deploy -n kube-system\n",
    "!kubectl create serviceaccount --namespace kube-system tiller\n",
    "!kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller\n",
    "!kubectl patch deploy --namespace kube-system tiller-deploy -p '{\"spec\":{\"template\":{\"spec\":{\"serviceAccount\":\"tiller\"}}}}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now install `seldon-core` on the new cluster using `helm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME:   seldon-core\n",
      "LAST DEPLOYED: Wed Oct  2 18:34:43 2019\n",
      "NAMESPACE: seldon-system\n",
      "STATUS: DEPLOYED\n",
      "\n",
      "RESOURCES:\n",
      "==> v1/ClusterRole\n",
      "NAME                          AGE\n",
      "seldon-operator-manager-role  0s\n",
      "\n",
      "==> v1/ClusterRoleBinding\n",
      "NAME                                 AGE\n",
      "seldon-operator-manager-rolebinding  0s\n",
      "\n",
      "==> v1/ConfigMap\n",
      "NAME           DATA  AGE\n",
      "seldon-config  1     0s\n",
      "\n",
      "==> v1/Pod(related)\n",
      "NAME                                  READY  STATUS             RESTARTS  AGE\n",
      "seldon-operator-controller-manager-0  0/1    ContainerCreating  0         0s\n",
      "\n",
      "==> v1/Secret\n",
      "NAME                                   TYPE    DATA  AGE\n",
      "seldon-operator-webhook-server-secret  Opaque  0     0s\n",
      "\n",
      "==> v1/Service\n",
      "NAME                                        TYPE       CLUSTER-IP     EXTERNAL-IP  PORT(S)  AGE\n",
      "seldon-operator-controller-manager-service  ClusterIP  10.110.207.29  <none>       443/TCP  0s\n",
      "webhook-server-service                      ClusterIP  10.97.95.144   <none>       443/TCP  0s\n",
      "\n",
      "==> v1/ServiceAccount\n",
      "NAME                              SECRETS  AGE\n",
      "seldon-core-seldon-core-operator  1        0s\n",
      "\n",
      "==> v1/StatefulSet\n",
      "NAME                                READY  AGE\n",
      "seldon-operator-controller-manager  0/1    0s\n",
      "\n",
      "==> v1beta1/CustomResourceDefinition\n",
      "NAME                                         AGE\n",
      "seldondeployments.machinelearning.seldon.io  0s\n",
      "\n",
      "\n",
      "NOTES:\n",
      "NOTES: TODO\n",
      "\n",
      "\n",
      "Waiting for 1 pods to be ready...\n",
      "partitioned roll out complete: 1 new pods have been updated...\n"
     ]
    }
   ],
   "source": [
    "!helm install \\\n",
    "    seldon-core-operator \\\n",
    "    --name seldon-core \\\n",
    "    --repo https://storage.googleapis.com/seldon-charts \\\n",
    "    --namespace seldon-system \\\n",
    "    --set usagemetrics.enabled=true \\\n",
    "    --set ambassador.enabled=true\n",
    "!kubectl rollout status statefulset.apps/seldon-operator-controller-manager -n seldon-system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we install `ambassador` which will allow us to reach the Seldon engine in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME:   ambassador\n",
      "LAST DEPLOYED: Wed Oct  2 18:52:59 2019\n",
      "NAMESPACE: default\n",
      "STATUS: DEPLOYED\n",
      "\n",
      "RESOURCES:\n",
      "==> v1/Deployment\n",
      "NAME        READY  UP-TO-DATE  AVAILABLE  AGE\n",
      "ambassador  0/3    3           0          0s\n",
      "\n",
      "==> v1/Pod(related)\n",
      "NAME                         READY  STATUS             RESTARTS  AGE\n",
      "ambassador-5c76696fcc-7rdlh  0/1    ContainerCreating  0         0s\n",
      "ambassador-5c76696fcc-m5ndq  0/1    ContainerCreating  0         0s\n",
      "ambassador-5c76696fcc-p6ddq  0/1    ContainerCreating  0         0s\n",
      "\n",
      "==> v1/Service\n",
      "NAME              TYPE          CLUSTER-IP     EXTERNAL-IP  PORT(S)                     AGE\n",
      "ambassador        LoadBalancer  10.103.89.133  <pending>    80:31527/TCP,443:31186/TCP  0s\n",
      "ambassador-admin  ClusterIP     10.111.105.26  <none>       8877/TCP                    0s\n",
      "\n",
      "==> v1/ServiceAccount\n",
      "NAME        SECRETS  AGE\n",
      "ambassador  1        0s\n",
      "\n",
      "==> v1beta1/ClusterRole\n",
      "NAME             AGE\n",
      "ambassador       0s\n",
      "ambassador-crds  0s\n",
      "\n",
      "==> v1beta1/ClusterRoleBinding\n",
      "NAME             AGE\n",
      "ambassador       0s\n",
      "ambassador-crds  0s\n",
      "\n",
      "\n",
      "NOTES:\n",
      "Congratuations! You've successfully installed Ambassador.\n",
      "\n",
      "For help, visit our Slack at https://d6e.co/slack or view the documentation online at https://www.getambassador.io.\n",
      "\n",
      "To get the IP address of Ambassador, run the following commands:\n",
      "NOTE: It may take a few minutes for the LoadBalancer IP to be available.\n",
      "     You can watch the status of by running 'kubectl get svc -w  --namespace default ambassador'\n",
      "\n",
      "  On GKE/Azure:\n",
      "  export SERVICE_IP=$(kubectl get svc --namespace default ambassador -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\n",
      "\n",
      "  On AWS:\n",
      "  export SERVICE_IP=$(kubectl get svc --namespace default ambassador -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')\n",
      "\n",
      "  echo http://$SERVICE_IP:\n",
      "\n",
      "Waiting for deployment \"ambassador\" rollout to finish: 0 of 3 updated replicas are available...\n",
      "Waiting for deployment \"ambassador\" rollout to finish: 1 of 3 updated replicas are available...\n",
      "Waiting for deployment \"ambassador\" rollout to finish: 2 of 3 updated replicas are available...\n",
      "deployment \"ambassador\" successfully rolled out\n"
     ]
    }
   ],
   "source": [
    "!helm install stable/ambassador --name ambassador --set crds.keep=false\n",
    "!kubectl rollout status deployment.apps/ambassador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward port\n",
    "\n",
    "Once the cluster has been created, we need to allow access from the outside to the `ambassador` gateway.\n",
    "One way to do this is to use the `kubectl port-forward` command.\n",
    "In particular, we will forward port `8003` of our local host to the cluster's gateway.\n",
    "\n",
    "This command needs to run constantly on the background, so **please make sure you run it on a separate terminal**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "kubectl \\\n",
    "    port-forward \\\n",
    "    $(kubectl get pods \\\n",
    "        -l app.kubernetes.io/name=ambassador -o jsonpath='{.items[0].metadata.name}') \\\n",
    "    8003:8080\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install Seldon Core Analytics\n",
    "\n",
    "Later, after we deploy the models, we will compare their performance using Seldon Core's integration with Prometheus and Grafana.\n",
    "For that part to work, we first need to install Grafana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME:   seldon-core-analytics\n",
      "LAST DEPLOYED: Wed Oct  2 20:19:38 2019\n",
      "NAMESPACE: default\n",
      "STATUS: DEPLOYED\n",
      "\n",
      "RESOURCES:\n",
      "==> v1/ConfigMap\n",
      "NAME                       DATA  AGE\n",
      "alertmanager-server-conf   1     0s\n",
      "grafana-import-dashboards  11    0s\n",
      "prometheus-rules           0     0s\n",
      "prometheus-server-conf     1     0s\n",
      "\n",
      "==> v1/Job\n",
      "NAME                            COMPLETIONS  DURATION  AGE\n",
      "grafana-prom-import-dashboards  0/1          0s        0s\n",
      "\n",
      "==> v1/Pod(related)\n",
      "NAME                                      READY  STATUS             RESTARTS  AGE\n",
      "alertmanager-deployment-db58649dd-mkcjn   0/1    ContainerCreating  0         0s\n",
      "grafana-prom-deployment-8564b575dd-gvpfk  0/1    ContainerCreating  0         0s\n",
      "grafana-prom-import-dashboards-xx9gl      0/1    ContainerCreating  0         0s\n",
      "prometheus-deployment-d57b5c748-fk5gp     0/1    Pending            0         0s\n",
      "prometheus-node-exporter-bcgkr            0/1    Pending            0         0s\n",
      "\n",
      "==> v1/Secret\n",
      "NAME                 TYPE    DATA  AGE\n",
      "grafana-prom-secret  Opaque  1     0s\n",
      "\n",
      "==> v1/Service\n",
      "NAME                      TYPE       CLUSTER-IP      EXTERNAL-IP  PORT(S)       AGE\n",
      "alertmanager              ClusterIP  10.96.90.189    <none>       80/TCP        0s\n",
      "grafana-prom              NodePort   10.111.192.89   <none>       80:30031/TCP  0s\n",
      "prometheus-node-exporter  ClusterIP  None            <none>       9100/TCP      0s\n",
      "prometheus-seldon         ClusterIP  10.109.148.211  <none>       80/TCP        0s\n",
      "\n",
      "==> v1/ServiceAccount\n",
      "NAME        SECRETS  AGE\n",
      "prometheus  1        0s\n",
      "\n",
      "==> v1beta1/ClusterRole\n",
      "NAME        AGE\n",
      "prometheus  0s\n",
      "\n",
      "==> v1beta1/ClusterRoleBinding\n",
      "NAME        AGE\n",
      "prometheus  0s\n",
      "\n",
      "==> v1beta1/DaemonSet\n",
      "NAME                      DESIRED  CURRENT  READY  UP-TO-DATE  AVAILABLE  NODE SELECTOR  AGE\n",
      "prometheus-node-exporter  1        1        0      1           0          <none>         0s\n",
      "\n",
      "==> v1beta1/Deployment\n",
      "NAME                     READY  UP-TO-DATE  AVAILABLE  AGE\n",
      "alertmanager-deployment  0/1    1           0          0s\n",
      "grafana-prom-deployment  0/1    1           0          0s\n",
      "prometheus-deployment    0/1    1           0          0s\n",
      "\n",
      "\n",
      "NOTES:\n",
      "NOTES: TODO\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!helm install seldon-core-analytics --name seldon-core-analytics \\\n",
    "     --repo https://storage.googleapis.com/seldon-charts \\\n",
    "     --set grafana_prom_admin_password=password \\\n",
    "     --set persistence.enabled=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access Grafana, it will be necessary to forward the port to the respective pod as we did previously to access the Seldon Core deployment.\n",
    "The credentials will be simply `admin` // `password`.\n",
    "\n",
    "This command needs to run constantly on the background, so **please make sure you run it on a separate terminal**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ kubectl port-forward \\\n",
    "    $(kubectl get pods \\\n",
    "        -l app=grafana-prom-server -o jsonpath='{.items[0].metadata.name}') \\\n",
    "    3000:3000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Deploy models\n",
    "\n",
    "Once the cluster is set up, the next step will to upload these models into a common repository and to deploy two `SeldonDeployment` specs to `k8s`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload models (optional)\n",
    "\n",
    "To make sure our `k8s` pods have access to the models we have just trained using `MLflow`, we will upload them into Google Cloud Storage. Note that to run these commands you need write access into the `gs://seldon-models` bucket and you need to have `gsutil` set up.\n",
    "\n",
    "We will upload both versions of the model to:\n",
    "\n",
    "- `gs://seldon-models/mlflow/model-a`\n",
    "- `gs://seldon-models/mlflow/model-b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://mlruns/0/169119a7fe1e4b31a746e891499552b0/artifacts/model/model.pkl [Content-Type=application/octet-stream]...\n",
      "Copying file://mlruns/0/169119a7fe1e4b31a746e891499552b0/artifacts/model/conda.yaml [Content-Type=application/octet-stream]...\n",
      "Copying file://mlruns/0/169119a7fe1e4b31a746e891499552b0/artifacts/model/MLmodel [Content-Type=application/octet-stream]...\n",
      "- [3 files][  1.1 KiB/  1.1 KiB]                                                \n",
      "Operation completed over 3 objects/1.1 KiB.                                      \n",
      "Copying file://mlruns/0/5a6be5a1ef844783a50a6577745dbdc3/artifacts/model/model.pkl [Content-Type=application/octet-stream]...\n",
      "Copying file://mlruns/0/5a6be5a1ef844783a50a6577745dbdc3/artifacts/model/conda.yaml [Content-Type=application/octet-stream]...\n",
      "Copying file://mlruns/0/5a6be5a1ef844783a50a6577745dbdc3/artifacts/model/MLmodel [Content-Type=application/octet-stream]...\n",
      "- [3 files][  1.1 KiB/  1.1 KiB]                                                \n",
      "Operation completed over 3 objects/1.1 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp -r mlruns/0/169119a7fe1e4b31a746e891499552b0/artifacts/model gs://seldon-models/mlflow/model-a\n",
    "!gsutil cp -r mlruns/0/5a6be5a1ef844783a50a6577745dbdc3/artifacts/model gs://seldon-models/mlflow/model-b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy specs\n",
    "\n",
    "We will deploy our A/B inference graph to our `k8s` cluster. As we can see below, we will route 50% of the traffic to each of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[04m\u001b[36m---\u001b[39;49;00m\n",
      "\u001b[94mapiVersion\u001b[39;49;00m: machinelearning.seldon.io/v1alpha2\n",
      "\u001b[94mkind\u001b[39;49;00m: SeldonDeployment\n",
      "\u001b[94mmetadata\u001b[39;49;00m:\n",
      "  \u001b[94mname\u001b[39;49;00m: wines-classifier\n",
      "\u001b[94mspec\u001b[39;49;00m:\n",
      "  \u001b[94mname\u001b[39;49;00m: wines-classifier\n",
      "  \u001b[94mpredictors\u001b[39;49;00m:\n",
      "  - \u001b[94mgraph\u001b[39;49;00m:\n",
      "      \u001b[94mchildren\u001b[39;49;00m: []\n",
      "      \u001b[94mimplementation\u001b[39;49;00m: MLFLOW_SERVER\n",
      "      \u001b[94mmodelUri\u001b[39;49;00m: gs://seldon-models/mlflow/model-a\n",
      "      \u001b[94mname\u001b[39;49;00m: wines-classifier\n",
      "    \u001b[94mname\u001b[39;49;00m: model-a\n",
      "    \u001b[94mreplicas\u001b[39;49;00m: 1\n",
      "    \u001b[94mtraffic\u001b[39;49;00m: 50\n",
      "  - \u001b[94mgraph\u001b[39;49;00m:\n",
      "      \u001b[94mchildren\u001b[39;49;00m: []\n",
      "      \u001b[94mimplementation\u001b[39;49;00m: MLFLOW_SERVER\n",
      "      \u001b[94mmodelUri\u001b[39;49;00m: gs://seldon-models/mlflow/model-b\n",
      "      \u001b[94mname\u001b[39;49;00m: wines-classifier\n",
      "    \u001b[94mname\u001b[39;49;00m: model-b\n",
      "    \u001b[94mreplicas\u001b[39;49;00m: 1\n",
      "    \u001b[94mtraffic\u001b[39;49;00m: 50\n",
      "seldondeployment.machinelearning.seldon.io/wines-classifier created\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ./serving/model-a-b.yaml\n",
    "!kubectl apply -f ./serving/model-a-b.yaml "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify these have been deployed by checking the pods and `SeldonDeployment` resources in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                READY   STATUS     RESTARTS   AGE\n",
      "ambassador-5c76696fcc-7rdlh                         1/1     Running    0          4m32s\n",
      "ambassador-5c76696fcc-m5ndq                         1/1     Running    0          4m32s\n",
      "ambassador-5c76696fcc-p6ddq                         1/1     Running    0          4m32s\n",
      "wines-classifier-model-a-77efeb1-76b468f4dc-969jv   0/2     Init:0/1   0          4s\n",
      "wines-classifier-model-b-77efeb1-64f6d4ddc-7rfcj    0/2     Init:0/1   0          4s\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME               AGE\n",
      "wines-classifier   4s\n"
     ]
    }
   ],
   "source": [
    "!kubectl get sdep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test models\n",
    "\n",
    "We will now run a sample query to test that the inference graph is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \u001b[94m\"data\"\u001b[39;49;00m: {\n",
      "        \u001b[94m\"names\"\u001b[39;49;00m: [],\n",
      "        \u001b[94m\"ndarray\"\u001b[39;49;00m: [\n",
      "            \u001b[34m5.554168965299016\u001b[39;49;00m\n",
      "        ]\n",
      "    },\n",
      "    \u001b[94m\"meta\"\u001b[39;49;00m: {\n",
      "        \u001b[94m\"metrics\"\u001b[39;49;00m: [],\n",
      "        \u001b[94m\"puid\"\u001b[39;49;00m: \u001b[33m\"oqotru9j2ga7h7gfcs8csqqhq6\"\u001b[39;49;00m,\n",
      "        \u001b[94m\"requestPath\"\u001b[39;49;00m: {\n",
      "            \u001b[94m\"wines-classifier\"\u001b[39;49;00m: \u001b[33m\"seldonio/mlflowserver_rest:0.2\"\u001b[39;49;00m\n",
      "        },\n",
      "        \u001b[94m\"routing\"\u001b[39;49;00m: {},\n",
      "        \u001b[94m\"tags\"\u001b[39;49;00m: {}\n",
      "    }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!http \\\n",
    "    --print b \\\n",
    "    localhost:8003/seldon/default/wines-classifier/api/v0.1/predictions \\\n",
    "    data:='{\\\n",
    "        \"names\": [\"fixed acidity\",\"volatile acidity\",\"citric acid\",\"residual sugar\",\"chlorides\",\"free sulfur dioxide\",\"total sulfur dioxide\",\"density\",\"pH\",\"sulphates\",\"alcohol\"], \\\n",
    "        \"ndarray\": [[7,0.27,0.36,20.7,0.045,45,170,1.001,3,0.45,8.8]] \\\n",
    "    }'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytics\n",
    "\n",
    "Now that we have both models running in production, we can analyse their performance using Seldon Core's integration with Prometheus and Grafana.\n",
    "To do so, we will iterate over the training set (which can be foud in `./training/wine-quality.csv`), making a request and sending the feedback of the prediction.\n",
    "\n",
    "Since the `/feedback` endpoint requires a `reward` signal (i.e. higher better), we will simulate one as\n",
    "\n",
    "$$\n",
    "  R(x_{n})\n",
    "    = \\begin{cases}\n",
    "        \\frac{1}{(y_{n} - f(x_{n}))^{2}} &, y_{n} \\neq f(x_{n}) \\\\\n",
    "        500 &, y_{n} = f(x_{n})\n",
    "      \\end{cases}\n",
    "$$\n",
    "\n",
    ", where $R(x_{n})$ is the reward for input point $x_{n}$, $f(x_{n})$ is our trained model and $y_{n}$ is the actual value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [5.031058953096899]\n",
       "1        [3.570741580101588]\n",
       "2        [70.55147231720159]\n",
       "3       [10.762015969288063]\n",
       "4       [10.762015969288063]\n",
       "                ...         \n",
       "4893     [375.9028525034419]\n",
       "4894    [1.8348875422756648]\n",
       "4895     [9.053003755655473]\n",
       "4896    [3.8999412500944777]\n",
       "4897     [22.25374886390087]\n",
       "Length: 4898, dtype: object"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from seldon_core.seldon_client import SeldonClient\n",
    "\n",
    "sc = SeldonClient(\n",
    "    gateway=\"ambassador\", \n",
    "    namespace=\"default\",\n",
    "    deployment_name='wines-classifier')\n",
    "\n",
    "df = pd.read_csv(\"./training/wine-quality.csv\")\n",
    "\n",
    "def _get_reward(y, y_pred):\n",
    "    if y == y_pred:\n",
    "        return 500    \n",
    "    \n",
    "    return 1 / np.square(y - y_pred)\n",
    "\n",
    "def _test_row(row):\n",
    "    input_features = row[:-1]\n",
    "    feature_names = input_features.index.to_list()\n",
    "    X = input_features.values.reshape(1, -1)\n",
    "    y = row[-1].reshape(1, -1)\n",
    "    \n",
    "    r = sc.predict(\n",
    "        data=X,\n",
    "        names=feature_names)\n",
    "    \n",
    "    y_pred = r.response.data.tensor.values\n",
    "    reward = _get_reward(y, y_pred)\n",
    "    sc.feedback(\n",
    "        prediction_request=r.request,\n",
    "        prediction_response=r.response,\n",
    "        reward=reward)\n",
    "    \n",
    "    return reward[0]\n",
    "\n",
    "df.apply(_test_row, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can visualise the Grafana dashboard below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Seldon Analytics](./images/seldon-analytics.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
